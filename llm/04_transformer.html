<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.15" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #22272e;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme')
      const systemDarkMode =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (userMode === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (userMode === 'dark' || systemDarkMode) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <title>语言模型内部的类神经网络transformer背后做了什么？ | Beholdia</title><meta name="description" content="一些文档">
    <link rel="preload" href="/assets/style-BQTB9GID.css" as="style"><link rel="stylesheet" href="/assets/style-BQTB9GID.css">
    <link rel="modulepreload" href="/assets/app-CeRtq0eM.js"><link rel="modulepreload" href="/assets/04_transformer.html-BbubjuiZ.js">
    <link rel="prefetch" href="/assets/get-started.html-CVTsyiYJ.js" as="script"><link rel="prefetch" href="/assets/index.html-t8k1ptyG.js" as="script"><link rel="prefetch" href="/assets/vocabulary.html-zutRdBJX.js" as="script"><link rel="prefetch" href="/assets/01_introduction.html-Cy-S3Hij.js" as="script"><link rel="prefetch" href="/assets/02_method.html-BzhpMpS1.js" as="script"><link rel="prefetch" href="/assets/03_train.html-DKYzNt_T.js" as="script"><link rel="prefetch" href="/assets/06_evaluation.html-Djvo5uA9.js" as="script"><link rel="prefetch" href="/assets/07_autoregressive.html-CcYOt2gp.js" as="script"><link rel="prefetch" href="/assets/08_speculative_decoding.html-BnENS3wL.js" as="script"><link rel="prefetch" href="/assets/neuron.html-BxBRGPA7.js" as="script"><link rel="prefetch" href="/assets/optimization.html-j21AmWSp.js" as="script"><link rel="prefetch" href="/assets/404.html-DD5vtWMw.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="vp-site-logo" src="https://vuejs.press/images/hero.png" alt="Beholdia"><span class="vp-site-name vp-hide-mobile" aria-hidden="true">Beholdia</span></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><nav class="vp-navbar-items vp-hide-mobile" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="Home"><!---->Home<!----></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/get-started.html" aria-label="Get Started"><!---->Get Started<!----></a></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="Japanese"><span class="title">Japanese</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="Japanese"><span class="title">Japanese</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/Japanese/vocabulary.html" aria-label="vocabulary"><!---->vocabulary<!----></a></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="llm"><span class="title">llm</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="llm"><span class="title">llm</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/01_introduction.html" aria-label="引入generative ai"><!---->引入generative ai<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/02_method.html" aria-label="不训练模型就强化语言模型的方法"><!---->不训练模型就强化语言模型的方法<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/03_train.html" aria-label="训练/学习的过程叫做最优化"><!---->训练/学习的过程叫做最优化<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link route-link-active auto-link" href="/llm/04_transformer.html" aria-label="语言模型内部的类神经网络transformer背后做了什么？"><!---->语言模型内部的类神经网络transformer背后做了什么？<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/06_evaluation.html" aria-label="评价模型"><!---->评价模型<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/07_autoregressive.html" aria-label="两种生成策略： AR &amp; NAR"><!---->两种生成策略： AR &amp; NAR<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/08_speculative_decoding.html" aria-label="加速所有语言模型生成速度的神奇外挂"><!---->加速所有语言模型生成速度的神奇外挂<!----></a></li><!--]--></ul></div></div><!--]--></nav><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><nav class="vp-navbar-items" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="Home"><!---->Home<!----></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/get-started.html" aria-label="Get Started"><!---->Get Started<!----></a></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="Japanese"><span class="title">Japanese</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="Japanese"><span class="title">Japanese</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/Japanese/vocabulary.html" aria-label="vocabulary"><!---->vocabulary<!----></a></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="llm"><span class="title">llm</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="llm"><span class="title">llm</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/01_introduction.html" aria-label="引入generative ai"><!---->引入generative ai<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/02_method.html" aria-label="不训练模型就强化语言模型的方法"><!---->不训练模型就强化语言模型的方法<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/03_train.html" aria-label="训练/学习的过程叫做最优化"><!---->训练/学习的过程叫做最优化<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link route-link-active auto-link" href="/llm/04_transformer.html" aria-label="语言模型内部的类神经网络transformer背后做了什么？"><!---->语言模型内部的类神经网络transformer背后做了什么？<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/06_evaluation.html" aria-label="评价模型"><!---->评价模型<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/07_autoregressive.html" aria-label="两种生成策略： AR &amp; NAR"><!---->两种生成策略： AR &amp; NAR<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/08_speculative_decoding.html" aria-label="加速所有语言模型生成速度的神奇外挂"><!---->加速所有语言模型生成速度的神奇外挂<!----></a></li><!--]--></ul></div></div><!--]--></nav><!--[--><!--]--><ul class="vp-sidebar-items"><!--[--><li><p tabindex="0" class="vp-sidebar-item vp-sidebar-heading">语言模型内部的类神经网络transformer背后做了什么？ <!----></p><!----></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div class="theme-default-content" vp-content><!--[--><!--]--><div><h1 id="语言模型内部的类神经网络transformer背后做了什么" tabindex="-1"><a class="header-anchor" href="#语言模型内部的类神经网络transformer背后做了什么"><span>语言模型内部的类神经网络transformer背后做了什么？</span></a></h1><p>语言模型是怎么做文字接龙的？[10]</p><h3 id="是什么" tabindex="-1"><a class="header-anchor" href="#是什么"><span>是什么？</span></a></h3><p>语言模型是一个函式，内部有很多未知的参数。可以用训练资料找出这些未知的参数。</p><h3 id="做了什么" tabindex="-1"><a class="header-anchor" href="#做了什么"><span>做了什么？</span></a></h3><p>语言模型做的事情就是文字接龙，文字接龙的能力是从大量的训练资料所学出来的。</p><h3 id="怎么做到的" tabindex="-1"><a class="header-anchor" href="#怎么做到的"><span>怎么做到的？</span></a></h3><p>语言模型背后用的是类神经网络的技术，也就是它背后的函式是一个类神经网络。 语言模型最常用的类神经网络的模型就是transformer。</p><p>语言模型使用的模型的演进：</p><ul><li>N-gram模型(深度学习不发达时)</li><li>Feed-forward Network模型（类神经网络技术）</li><li>Recurrent Neural Network模型（类神经网络技术）</li><li>transformer模型(类神经网络技术)</li></ul><p><img src="/assets/transformer-DOjlR9sK.png" alt="transformer"></p><h2 id="第一步-tokenization" tabindex="-1"><a class="header-anchor" href="#第一步-tokenization"><span>第一步 tokenization</span></a></h2><p>把文字转换成token。</p><p>语言模型是以token作为单位对文字进行处理。 语言模型输入和输出的单位都是以token作为单位的。 把一句话给语言模型，它做的第一件事是把这句话转成token的sequence。</p><p><strong>如何转换：</strong></p><p>怎么把一句话转成tokens呢？怎么知道一个语言里面有哪些token呢？我们要先准备一个token列表。一开始在打造语言模型的时候，就会根据对语言的理解，准备一个token列表——也就是语言模型处理文字的基本单位。不同的语言模型，token的列表是不一样的。</p><p><strong>自动取得token：</strong></p><ul><li>有一个演算法叫byte pair encoding (BPE)，可以从大量文字里面找出常常出现的pattern，把这些pattern当作token。</li><li>还有其他找token的方法。</li></ul><p>这部分没有要训练的参数，是一个人事先定好的module。<!----><br> 每个语言模型用的token都是不一样的。 <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener noreferrer">查看openai的token列表</a>。</p><p><img src="/assets/tokenization-CSGbI4NK.png" alt="tokenization"></p><h2 id="第二步-input-layer" tabindex="-1"><a class="header-anchor" href="#第二步-input-layer"><span>第二步 input layer</span></a></h2><p>理解token，知道每一个token是什么意思</p><h3 id="理解token-语意-embedding-token-embedding" tabindex="-1"><a class="header-anchor" href="#理解token-语意-embedding-token-embedding"><span>理解token 语意/embedding/token embedding</span></a></h3><ul><li><p>做了什么？<br> 需要把每一个token用一个向量来表示。每一个token丢尽这个模组之后，每一个token变成一串数字，每一个token会变成一个向量。 把token变成向量的<strong>过程</strong>叫做embedding。<br> 我们把每一个token表示成一个向量，有时候直接称呼这些<strong>向量</strong>为embedding。<br> 语言越相近的token，它的embedding距离就越接近。（见下图，比如动物就很接近）<br> 由此，接下来处理的时候就能够知道token和token之间的关联性。</p></li><li><p>怎么做？<br> 怎么把相邻的token给它相近的embedding呢？ 有一个token embedding的列表。 transformer 里有一个table，存了所有的token 跟每一个token所对应的embedding（向量）。 做embedding就是查表。</p></li><li><p>哪里来？<br> 这个表是从哪里来的呢？怎么知道每一个token对应的向量是什么呢？token所对应的向量就是语言模型里面的参数，就是通过大量训练资料找的参数的一部分。 token的embedding<!----><strong>它。<br> 但是这些embedding有一些限制，就是它</strong>没有考虑上下文**。同一个token它的embedding就是固定的。<br> bank不管表示哪一个意思，它的embedding都是一样的。<br> 第三阶段才考虑上下文。<br></p></li></ul><p><img src="/assets/embedding-BHPx9N1v.png" alt="embedding"><img src="/assets/embedding2-BExVCz4M.png" alt="embedding"></p><h3 id="理解token-位置-positional-embedding" tabindex="-1"><a class="header-anchor" href="#理解token-位置-positional-embedding"><span>理解token 位置/positional embedding</span></a></h3><p>上面考虑了语意的资讯，其实还需要考虑位置的资讯。我们需要知道每一个token是在句子里的哪一个位置。<br> 因为同一个token，在句子里面不同位置，可能会代表不同的意思。<br> 怎么把位置的资讯加到token的embedding里面去呢？一个可能的做法是为每一个位置也设定一个向量：positional embedding。<br> 那么一个token除了表示语义的embedding之后，还要再加上表示位置的embedding。<br> positional embedding可以<!---->，也可以<!---->：作为参数的一部分。<br><img src="/assets/positional_embedding-CMv205MU.png" alt="positional_embedding"></p><h2 id="第三步-attention-contextualized-token-embedding" tabindex="-1"><a class="header-anchor" href="#第三步-attention-contextualized-token-embedding"><span>第三步 attention / contextualized token embedding</span></a></h2><p>理解上下文 经过这一模组之后，&quot;苹果电脑&quot;的&quot;果&quot;和&quot;来吃苹果&quot;的&quot;果&quot;的embedding变得不一样了。 这种有考虑上下文的embedding，叫做contextualized token embedding。这个时候同一个token语意不一样。</p><p>attention is all you need发现了不用搭配RNN就有很好的效果。</p><ul><li>attention做了什么 <ul><li>attention做的事情，就是输入一排向量，在这排向量里面每一个向量都代表一个token。它会输出另外一排一样长度的向量。对每一个token来说就是把上下文的资讯，加进它的embedding里面去。具体看一下attention是做了什么事情，使输入的embedding变成输出的embedding。</li><li>计算相关性，再做weighted sum。<br> 从整个句子里找出相关的token，有一个计算相关性的模组，把某一个token的embedding读进去，然后再把句子里的某一个embedding读进去，做一个分数，这个分数代表这两个token之间的关联性，这两个token embedding有多相近，这个分数叫做attention weight。把句子里的全部其他embedding都分别作出attention weight，包括自己本身也要做一个相关性的分数。句子里的每一个向量都要做相关性，再把这些分数加起来。就是输出。 计算相关性的模组也是一个函式。是通过训练资料学到的。</li></ul></li></ul><p><img src="/assets/attention01-BvDY2R0d.png" alt="attention01"><img src="/assets/attention02-BvoQTSkA.png" alt="attention02"><img src="/assets/attention03-Clik_6hk.png" alt="attention03"></p><!----><h3 id="casual-attention" tabindex="-1"><a class="header-anchor" href="#casual-attention"><span>casual attention</span></a></h3><ul><li>但实际上只会考虑每一个token前面的那些token。计算attention weight的时候，只会管它左边的token。这样叫做casual attention。</li><li>为什么呢？<em>再看一下</em></li></ul><p><img src="/assets/casual_attention-i398Hls-.png" alt="casual_attention"></p><h3 id="multi-head-attention" tabindex="-1"><a class="header-anchor" href="#multi-head-attention"><span>multi-head attention</span></a></h3><p>attention里面还有一个特别的设计。multi-head attention。 语言模型用的都是multi-head attention。 相关性不止一种。例如 dog cat ,dog bark。所以需要多个计算关联性的模组。不同的计算关联性的模组，都有自己的参数，这些参数都是透过训练资料学出来的。它们会算出不同的attention weight。</p><p>然后根据每一组attention weight 做weighted sum。所以实际上attention mudual<strong>不止一组输出</strong>。根据每一种关联性都会给我们一组输出。我们要把这些输出整合起来，我们要用到另一个模组，叫做feed forward network。</p><p><img src="/assets/multi_head_attention-CBebTJHg.png" alt="multi_head_attention"></p><h2 id="第四步-feed-forward" tabindex="-1"><a class="header-anchor" href="#第四步-feed-forward"><span>第四步 feed forward</span></a></h2><p>feed forward network会把多组输出综合考虑一下，最终给出一个向量。</p><p>feedforward模组也是一个neural network。里面也有很多层很多参数。参数也是透过训练资料学习出来的。 attention的模组加上feed forward的模组组合起来，就是一个transformer的block。是transformer内的基本单位。</p><p><img src="/assets/transformer_block-CZIZwny3.png" alt="transformer_block"></p><h2 id="第三四步多次-transformer-block" tabindex="-1"><a class="header-anchor" href="#第三四步多次-transformer-block"><span>第三四步多次 transformer block</span></a></h2><ul><li>一个transformer里面，有多个transformer block。</li><li>总结：一开始，我们通过第一个模组做tokenization，第二个模组是token embedding，得到第一组的token embedding后，就会通过transformer的block，里面有attention，有 feedforward network，你会得到另外一组embedding，这组embedding是考虑过上下文的embedding。</li><li>光通过一个transformer block是不够的，会把它再通过下一个transformer block，好几次。每一个transformer block会称为一个layer（虽然实际上如果从neural network的角度看，一个transformer block里面，也已经有很多层的neural network ）。所以，输入的embedding，会通过多个transformer block ，一直到通过最后一个transformer block之后，把句尾的这一个向量拿出来，再把句尾的向量，通过一个output layer。</li></ul><h2 id="第五步-output-layer" tabindex="-1"><a class="header-anchor" href="#第五步-output-layer"><span>第五步 output layer</span></a></h2><ul><li>output layer也是一个function，output layer里面有一个linear transform ，有一个softmax。然后会得到一个几率的分布。 就是说，transformer block最后一层的最后一个输出，丢给另外一个模组，这个模组，会输出一个机率分布给我们。</li></ul><p>一个句子从输入到transformer，一直到最终输出一个机率分布，代表了下一个token，应该接哪一个token的几率。</p><h2 id="为什么处理超长文本会是挑战" tabindex="-1"><a class="header-anchor" href="#为什么处理超长文本会是挑战"><span>为什么处理超长文本会是挑战</span></a></h2><p>如果输入100k，对语言模型来说，100k个token需要两两之间计算attention。计算attention weight的次数和文本token长度的平方成正比。 研究方向……</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="vp-meta-item last-updated"><span class="meta-item-label">Last Updated: </span><!----></div><div class="vp-meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: yuanbihe@outlook.com">Beholdia</span><!----><!--]--><!--]--></span></div></div></footer><!----><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-CeRtq0eM.js" defer></script>
  </body>
</html>
