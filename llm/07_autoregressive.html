<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.15" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #22272e;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme')
      const systemDarkMode =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (userMode === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (userMode === 'dark' || systemDarkMode) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <title>两种生成策略： AR & NAR | Beholdia</title><meta name="description" content="一些文档">
    <link rel="preload" href="/assets/style-BQTB9GID.css" as="style"><link rel="stylesheet" href="/assets/style-BQTB9GID.css">
    <link rel="modulepreload" href="/assets/app-CeRtq0eM.js"><link rel="modulepreload" href="/assets/07_autoregressive.html-CcYOt2gp.js">
    <link rel="prefetch" href="/assets/get-started.html-CVTsyiYJ.js" as="script"><link rel="prefetch" href="/assets/index.html-t8k1ptyG.js" as="script"><link rel="prefetch" href="/assets/vocabulary.html-zutRdBJX.js" as="script"><link rel="prefetch" href="/assets/01_introduction.html-Cy-S3Hij.js" as="script"><link rel="prefetch" href="/assets/02_method.html-BzhpMpS1.js" as="script"><link rel="prefetch" href="/assets/03_train.html-DKYzNt_T.js" as="script"><link rel="prefetch" href="/assets/04_transformer.html-BbubjuiZ.js" as="script"><link rel="prefetch" href="/assets/06_evaluation.html-Djvo5uA9.js" as="script"><link rel="prefetch" href="/assets/08_speculative_decoding.html-BnENS3wL.js" as="script"><link rel="prefetch" href="/assets/neuron.html-BxBRGPA7.js" as="script"><link rel="prefetch" href="/assets/optimization.html-j21AmWSp.js" as="script"><link rel="prefetch" href="/assets/404.html-DD5vtWMw.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="vp-site-logo" src="https://vuejs.press/images/hero.png" alt="Beholdia"><span class="vp-site-name vp-hide-mobile" aria-hidden="true">Beholdia</span></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><nav class="vp-navbar-items vp-hide-mobile" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="Home"><!---->Home<!----></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/get-started.html" aria-label="Get Started"><!---->Get Started<!----></a></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="Japanese"><span class="title">Japanese</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="Japanese"><span class="title">Japanese</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/Japanese/vocabulary.html" aria-label="vocabulary"><!---->vocabulary<!----></a></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="llm"><span class="title">llm</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="llm"><span class="title">llm</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/01_introduction.html" aria-label="引入generative ai"><!---->引入generative ai<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/02_method.html" aria-label="不训练模型就强化语言模型的方法"><!---->不训练模型就强化语言模型的方法<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/03_train.html" aria-label="训练/学习的过程叫做最优化"><!---->训练/学习的过程叫做最优化<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/04_transformer.html" aria-label="语言模型内部的类神经网络transformer背后做了什么？"><!---->语言模型内部的类神经网络transformer背后做了什么？<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/06_evaluation.html" aria-label="评价模型"><!---->评价模型<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link route-link-active auto-link" href="/llm/07_autoregressive.html" aria-label="两种生成策略： AR &amp; NAR"><!---->两种生成策略： AR &amp; NAR<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/08_speculative_decoding.html" aria-label="加速所有语言模型生成速度的神奇外挂"><!---->加速所有语言模型生成速度的神奇外挂<!----></a></li><!--]--></ul></div></div><!--]--></nav><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><nav class="vp-navbar-items" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="Home"><!---->Home<!----></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/get-started.html" aria-label="Get Started"><!---->Get Started<!----></a></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="Japanese"><span class="title">Japanese</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="Japanese"><span class="title">Japanese</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/Japanese/vocabulary.html" aria-label="vocabulary"><!---->vocabulary<!----></a></li><!--]--></ul></div></div><div class="vp-navbar-item"><div class="vp-navbar-dropdown-wrapper"><button class="vp-navbar-dropdown-title" type="button" aria-label="llm"><span class="title">llm</span><span class="arrow down"></span></button><button class="vp-navbar-dropdown-title-mobile" type="button" aria-label="llm"><span class="title">llm</span><span class="right arrow"></span></button><ul style="display:none;" class="vp-navbar-dropdown"><!--[--><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/01_introduction.html" aria-label="引入generative ai"><!---->引入generative ai<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/02_method.html" aria-label="不训练模型就强化语言模型的方法"><!---->不训练模型就强化语言模型的方法<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/03_train.html" aria-label="训练/学习的过程叫做最优化"><!---->训练/学习的过程叫做最优化<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/04_transformer.html" aria-label="语言模型内部的类神经网络transformer背后做了什么？"><!---->语言模型内部的类神经网络transformer背后做了什么？<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/06_evaluation.html" aria-label="评价模型"><!---->评价模型<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link route-link-active auto-link" href="/llm/07_autoregressive.html" aria-label="两种生成策略： AR &amp; NAR"><!---->两种生成策略： AR &amp; NAR<!----></a></li><li class="vp-navbar-dropdown-item"><a class="route-link auto-link" href="/llm/08_speculative_decoding.html" aria-label="加速所有语言模型生成速度的神奇外挂"><!---->加速所有语言模型生成速度的神奇外挂<!----></a></li><!--]--></ul></div></div><!--]--></nav><!--[--><!--]--><ul class="vp-sidebar-items"><!--[--><li><p tabindex="0" class="vp-sidebar-item vp-sidebar-heading">两种生成策略： AR &amp; NAR <!----></p><!----></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div class="theme-default-content" vp-content><!--[--><!--]--><div><h1 id="两种生成策略-ar-nar" tabindex="-1"><a class="header-anchor" href="#两种生成策略-ar-nar"><span>两种生成策略： AR &amp; NAR</span></a></h1><p>[15]</p><p><img src="/assets/generative_ai-BFPDt81u.png" alt="generative ai"></p><p>世界上token 有三万多。<br> 像素更多，由pixel构成。<br> 声音更多，声音由取样点（sample）所构成，每一秒多少取样点由取样解析度构成。</p><p>生成式人工智慧的本质：给一个条件，把基本单位用正确的排序组合起来。</p><p>有两种生成策略：</p><h2 id="自回归模型-autoregressive" tabindex="-1"><a class="header-anchor" href="#自回归模型-autoregressive"><span>自回归模型 autoregressive</span></a></h2><ul><li>对话的AI ：文字接龙，文字接龙的生成方式 autoregressive generation（AR）。在文字上很成功。</li><li>影像呢？ 可以用像素接龙。</li><li>声音可以用sample接龙。</li></ul><p>但事实上，影像和声音不是以autoregressive generation方式生成的。</p><p>这个策略有限制。按部就班、时间较长。 <img src="/assets/ar01-Cs0toj3z.png" alt="ar01"><img src="/assets/ar02-DLBFjvcZ.png" alt="ar02"></p><h2 id="非自回归模型-non-autoregressive" tabindex="-1"><a class="header-anchor" href="#非自回归模型-non-autoregressive"><span>非自回归模型 non-autoregressive</span></a></h2><ul><li>文字也可以non-auto regressive。可以先预测要生成多少字。或者是生成固定长度。</li><li>为什么文字总是用autoregressive呢？因为NAR的品质不好。独立生成时有很多不同想法。multi-modality problem。</li><li>例如NAR会出现“李宏毅是演授”问题。AR不会出现这个问题。</li></ul><p><img src="/assets/nar01-CE0aMTTF.png" alt="nar01"></p><h2 id="ar-nar" tabindex="-1"><a class="header-anchor" href="#ar-nar"><span>AR+NAR</span></a></h2><h3 id="从简到繁" tabindex="-1"><a class="header-anchor" href="#从简到繁"><span>从简到繁</span></a></h3><ul><li>精简版本不需要是人可以看懂的。可以拿encoder压缩图片。</li><li>encoder就是一个类神经网络。decoder可以看得懂压缩后的结果，并还原。</li><li>压缩后的图片，压缩程度是16*16 =&gt;1。</li><li>encoder decoder都可以是类神经网络，机器学习训练得到。这个方法叫：auto-encoder 收集一大堆图片，encoder压缩decoder释放，前后越接近越好。用机器学习的方式找出参数(这样就得到了一个压缩和解压工具)。</li><li>autoregressive只要产生压缩版本，丢进decoder，也就是Non-autoregressive的model 解压。</li><li>autoregressive model只要生成比较简单的东西，decoder是一个non-autoregressive的model,可以快速生成。两者取长补短。</li></ul><p><img src="/assets/arnar01-DmAxFGq5.png" alt="arnar01"></p><p><img src="/assets/arnar02-BaVBkj6U.png" alt="arnar02"></p><!----><h3 id="多次生成non-autoregressive" tabindex="-1"><a class="header-anchor" href="#多次生成non-autoregressive"><span>多次生成non-autoregressive</span></a></h3><p>另外一个克服non-autorefressive的方法就是多次生成non-autoregressive。分成好几个阶段。 多次生成，脑补有限。</p><ul><li>小图到大图</li></ul><p><img src="/assets/arnar03-IXFFJmA9.png" alt="arnar03"></p><ul><li>有杂讯到无杂讯 有杂讯-&gt;无杂讯的过程：图像生成中鼎鼎大名的diffusion model 。即 生成的过程拆解成生成不同版本的东西，每一个版本比前一个更加清晰。它就是autoregressive和non autoregressive的结合。</li></ul><p><img src="/assets/arnar04-CIYqRy0u.png" alt="arnar04"></p><ul><li>把不对的地方涂掉</li></ul><p><img src="/assets/arnar05-BaZTwpnO.png" alt="arnar05"></p><p>那么前面那个autoregressive的生成会不会还是很久呢？ 可以把前面autoregressive的生成直接换成一串non-autoregressive的生成。压缩的第一版-第二版。 也就是一堆non-autoregressive 生成一个人看不懂的压缩，再丢尽decoder生成最后的图片。</p><p><img src="/assets/arnar06-BxM-cIYM.png" alt="arnar06"></p><!----><h2 id="小结" tabindex="-1"><a class="header-anchor" href="#小结"><span>小结</span></a></h2><p><img src="/assets/arnar07-DspVvyiX.png" alt="arnar07"></p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="vp-meta-item last-updated"><span class="meta-item-label">Last Updated: </span><!----></div><div class="vp-meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: yuanbihe@outlook.com">Beholdia</span><!----><!--]--><!--]--></span></div></div></footer><!----><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-CeRtq0eM.js" defer></script>
  </body>
</html>
