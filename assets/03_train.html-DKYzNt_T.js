import{_ as e,c as n,o as i,a}from"./app-CeRtq0eM.js";const r="/assets/instruction_fine_tuning-84jnlIuJ.png",l="/assets/train-CVXsT_m4.png",t="/assets/seektasks-BL7OgNGg.png",s="/assets/RL-B_wBOY6_.png",p="/assets/rewardmodel-ASnQJRqD.png",o="/assets/3steps-QLL8XbVh.png",d={},c=a('<h1 id="训练-学习的过程叫做最优化" tabindex="-1"><a class="header-anchor" href="#训练-学习的过程叫做最优化"><span>训练/学习的过程叫做最优化</span></a></h1><p>大型语言文字模型就是文字接龙。 [6~8]</p><h2 id="训练-学习-最优化" tabindex="-1"><a class="header-anchor" href="#训练-学习-最优化"><span>训练/学习 ：最优化</span></a></h2><p><em>这个注意再看一下</em></p><h2 id="optimization-6-7" tabindex="-1"><a class="header-anchor" href="#optimization-6-7"><span>optimization [6~7]</span></a></h2><ul><li>失败：最优化方法不行。换一组hyper parameter再上一次。（需要算力）</li><li>成功 <ul><li>训练成功，测试失败：overfitting。 <ul><li>如何找到合理的超参？1.增加训练资料的多样性。2.找一组比较好的初始参数。</li><li>如果初始参数（第一组参数，从这里开始最优化）是随机生产的：这种最佳化的方法是train from scratch 。</li><li>最后得到的参数会和初始参数比较接近。</li></ul></li></ul></li></ul><p>需要多少文字才够学会文字接龙？需要1.语言知识2.世界知识</p><h2 id="第一阶段-self-supervised-learning-pretrain" tabindex="-1"><a class="header-anchor" href="#第一阶段-self-supervised-learning-pretrain"><span>第一阶段 self-supervised learning / pretrain</span></a></h2><ul><li>学了大量资料，但是回答不受控。叫做pretrain 也叫fundation model 预训练 .通过网爬网络资料，人类介入很少，叫做self-supervised learning.</li></ul><h2 id="第二阶段-supervised-learning-instruction-fine-tuning" tabindex="-1"><a class="header-anchor" href="#第二阶段-supervised-learning-instruction-fine-tuning"><span>第二阶段 supervised learning / instruction fine-tuning</span></a></h2><ul><li>用人类准备的资料来学习。叫做 instruction fine-tuning。这些训练资料要耗费大量的人力，就是要资料标注。通过耗费大量人力的资料标注的资料 训练出来的模型，训练过程叫做supervised learning fine-tuning <img src="'+r+'" alt="instruction-fine-tuning"></li></ul><p>只从人类老师学是不够的。人力很贵，无法收集太多资料。 所以，今天这些大型语言模型可以成功的关键，是使用了第一阶段使用到的参数，作为第二阶段的初始参数。第二阶段的参数不会和第一阶段参数差太多。所以这个步骤又叫做fine-tuning。如果我们把有用人类标注的资料学习当作真正的学习，那透过任何资料、网络上爬到的资料学出来的这些学习的这个过程叫做预训练pre-train。</p><p><img src="'+l+'" alt="train"></p><h2 id="adapter" tabindex="-1"><a class="header-anchor" href="#adapter"><span>adapter</span></a></h2><p>会不会第二个最佳化的过程最后找出来的参数还是跟初始参数很不一样呢？ 为了避免这个问题，我们就使用adapter。(LoRA就是Adapter的一种。)</p><ul><li>Adapter的概念：假设我们已经有初始参数了，第二阶段最佳化的时候不再动初始参数，而是在原有参数后面多加几项参数。</li><li>Adapter的优点：减少运算量（如果不用LoRa直接找全部的参数，会算很久，用了Adapter只要找少量参数，运算量少，可以在免费版colab上做）</li></ul><p>有一个好的pre-train的模型，在instruction fine-tuning的时候，就可以有很强的举一反三的能力。</p><h2 id="fine-tuning" tabindex="-1"><a class="header-anchor" href="#fine-tuning"><span>fine-tuning</span></a></h2><p>fine-tuning的路线分成了两条：</p><ul><li>1.打造一堆专才（bert）</li><li>2.打造通才：收集一大堆标注资料，涵盖各式各样的任务。对模型做instruction fine-tuning ，让模型能做各种任务。训练过的任务越多，在没看多的任务上正确量越高。尤其是小模型，进步是尤其显著的。</li></ul><p>2020的GPT2 GPT3就是个pretrain-model。回答不受控制。 后来instruct GPT就是对GPT3做了fine-tuning。利用的资料是过去gpt用户的真实的问题。所以品质比其它的好很多。</p><p>instruct fine-tuning不需要大量的资料。llama2 lima发现了兵贵精不贵多。quality is all you need。</p><p>但是普通人没有资料。可以向GPT要一个逆向工程。 有了llama之后，人人都可以fine tuning大语言模型的时代就开始了。<br><img src="'+t+'" alt="seektasks"></p><h2 id="第三阶段-reinforce-learning-8" tabindex="-1"><a class="header-anchor" href="#第三阶段-reinforce-learning-8"><span>第三阶段 reinforce learning [8]</span></a></h2><p><img src="'+s+'" alt="RL"></p><h3 id="rlhf-reinforce-learning-from-human-feedback" tabindex="-1"><a class="header-anchor" href="#rlhf-reinforce-learning-from-human-feedback"><span>RLHF reinforce learning from human feedback</span></a></h3><p>让模型和使用者互动。reinforce learning from human feedback（RLHF）。 本质上就是人觉得好的答案（让人来排序好坏），就提高他出现的几率。 RLHF VS instruction fine-tuning：</p><ul><li>从人类产生训练资料的角度来看，与第二阶段相比，写出正确答案不容易，但判断哪一个好相对容易。</li><li>从模型学习的角度来看，RLHF比较有机会让语言模型通判考量它生成的结果。</li><li>都是alignment RLHF只问结果不问过程，instruction fine-tuning相反</li></ul><p>增强式学习中，chatgpt微调参数的演算法叫做PPO。 下围棋alpha go也是一种增强式学习，只是基于规则且人类介入更少，可以自我训练。</p><h3 id="reward-model-虚拟人类" tabindex="-1"><a class="header-anchor" href="#reward-model-虚拟人类"><span>reward model / 虚拟人类</span></a></h3><p>如何更有效地利用人类地回馈？ 用人类的回馈创造虚拟的人类，语言模型需要回馈的时候，它可以问虚拟的人类。让语言模型想象人类，觉得什么是好的。 语言模型想象出来的人类：回馈模型。reward model. 怎么训练回馈模型？当我们把问题和答案给回馈模型的时候，它会给一个分数。 回馈模型的利用：</p><ul><li>1.给很多答案让它打分。选一个最好的。(不改变模型，只是选择)</li><li>2.更常见的利用方式：让语言模型直接对回馈模型进行学习。如果回馈模型给的分数低，就微调参数减低产生这个答案的几率，如果分数高，也调参。（改变了模型，调参了）</li></ul><p><em>这个注意再看一下</em><img src="'+p+'" alt="rewardmodel"></p><p>但是过度跟虚拟人类学习是有害的。DPO和KTO是一些取代虚拟老师的方法。</p><h3 id="rlaif" tabindex="-1"><a class="header-anchor" href="#rlaif"><span>RLAIF</span></a></h3><p>直接拿语言模型评价另外一个语言模型输出的好坏：RLAIF。 也可以是自己评价。语言模型有反省的能力，所以有机会让语言模型自己给自己提供回馈。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>第二阶段和第三阶段引入了人类老师，所以它们也可以叫alignment。（对齐人类的偏好和需求。） <img src="'+o+'" alt="3steps"></p>',38),u=[c];function h(f,g){return i(),n("div",null,u)}const _=e(d,[["render",h],["__file","03_train.html.vue"]]),v=JSON.parse('{"path":"/llm/03_train.html","title":"训练/学习的过程叫做最优化","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"训练/学习 ：最优化","slug":"训练-学习-最优化","link":"#训练-学习-最优化","children":[]},{"level":2,"title":"optimization [6~7]","slug":"optimization-6-7","link":"#optimization-6-7","children":[]},{"level":2,"title":"第一阶段 self-supervised learning / pretrain","slug":"第一阶段-self-supervised-learning-pretrain","link":"#第一阶段-self-supervised-learning-pretrain","children":[]},{"level":2,"title":"第二阶段 supervised learning / instruction fine-tuning","slug":"第二阶段-supervised-learning-instruction-fine-tuning","link":"#第二阶段-supervised-learning-instruction-fine-tuning","children":[]},{"level":2,"title":"adapter","slug":"adapter","link":"#adapter","children":[]},{"level":2,"title":"fine-tuning","slug":"fine-tuning","link":"#fine-tuning","children":[]},{"level":2,"title":"第三阶段 reinforce learning [8]","slug":"第三阶段-reinforce-learning-8","link":"#第三阶段-reinforce-learning-8","children":[{"level":3,"title":"RLHF reinforce learning from human feedback","slug":"rlhf-reinforce-learning-from-human-feedback","link":"#rlhf-reinforce-learning-from-human-feedback","children":[]},{"level":3,"title":"reward model / 虚拟人类","slug":"reward-model-虚拟人类","link":"#reward-model-虚拟人类","children":[]},{"level":3,"title":"RLAIF","slug":"rlaif","link":"#rlaif","children":[]}]},{"level":2,"title":"总结","slug":"总结","link":"#总结","children":[]}],"git":{"updatedTime":1753262892000,"contributors":[{"name":"Beholdia","email":"yuanbihe@outlook.com","commits":1}]},"filePathRelative":"llm/03_train.md"}');export{_ as comp,v as data};
