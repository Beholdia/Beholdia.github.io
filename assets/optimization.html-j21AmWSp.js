import{_ as i,c as a,o as n,a as t}from"./app-CeRtq0eM.js";const e={},l=t('<h1 id="optimization失败了怎么办" tabindex="-1"><a class="header-anchor" href="#optimization失败了怎么办"><span>optimization失败了怎么办</span></a></h1><h2 id="_1-陷入critical-point-gradient为0-微分为0的点" tabindex="-1"><a class="header-anchor" href="#_1-陷入critical-point-gradient为0-微分为0的点"><span>1.陷入critical point（gradient为0/微分为0的点）</span></a></h2><ul><li>saddle point</li><li>local minima</li></ul><h4 id="判断是哪种critical-point" tabindex="-1"><a class="header-anchor" href="#判断是哪种critical-point"><span>判断是哪种critical point</span></a></h4><ul><li>数学</li></ul><h4 id="解决办法" tabindex="-1"><a class="header-anchor" href="#解决办法"><span>解决办法</span></a></h4><ul><li>batch批次 <ul><li>small size 的batch 虽然noisy但training效果好。full batch更可能到saddle point或者sharp minima。</li></ul></li><li>动量momentum</li></ul><h2 id="_2-陷入来回震荡" tabindex="-1"><a class="header-anchor" href="#_2-陷入来回震荡"><span>2.陷入来回震荡</span></a></h2><p>不一定是critical point的问题。要确认下loss不再下降的时候，gradient是不是真的变得很小了。</p><h4 id="解决办法-1" tabindex="-1"><a class="header-anchor" href="#解决办法-1"><span>解决办法</span></a></h4><ul><li>adaptive learning rate <ul><li>学习率η除以过去g的根均方（used in adagrad） adagrade反应比较慢，因为它看前面的平均的</li><li>RMSProp 可以调整是新的gradient比较重要还是之前的平均的比较重要，α会决定最新的g，他有多重要。 反应较快</li></ul></li></ul><p>今天最常见的optimization（optimizer）的策略就是，adam。RMSProp+momentum</p><h2 id="_3-小步子走多了-learning-rate会井喷。虽然会回来。" tabindex="-1"><a class="header-anchor" href="#_3-小步子走多了-learning-rate会井喷。虽然会回来。"><span>3.小步子走多了，learning rate会井喷。虽然会回来。</span></a></h2><h4 id="解决办法-2" tabindex="-1"><a class="header-anchor" href="#解决办法-2"><span>解决办法</span></a></h4><ul><li>learning rate scheduling。 <ul><li>随着参数不断update，我们让η越来越小。</li><li>warm up（used in residual network）</li></ul></li></ul><p>loss function 本身也很重要！</p>',16),r=[l];function o(s,c){return n(),a("div",null,r)}const p=i(e,[["render",o],["__file","optimization.html.vue"]]),h=JSON.parse('{"path":"/ml2021/optimization.html","title":"optimization失败了怎么办","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"1.陷入critical point（gradient为0/微分为0的点）","slug":"_1-陷入critical-point-gradient为0-微分为0的点","link":"#_1-陷入critical-point-gradient为0-微分为0的点","children":[]},{"level":2,"title":"2.陷入来回震荡","slug":"_2-陷入来回震荡","link":"#_2-陷入来回震荡","children":[]},{"level":2,"title":"3.小步子走多了，learning rate会井喷。虽然会回来。","slug":"_3-小步子走多了-learning-rate会井喷。虽然会回来。","link":"#_3-小步子走多了-learning-rate会井喷。虽然会回来。","children":[]}],"git":{"updatedTime":null,"contributors":[]},"filePathRelative":"ml2021/optimization.md"}');export{p as comp,h as data};
